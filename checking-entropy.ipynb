{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = list()\n",
    "data_test = list()\n",
    "\n",
    "# Extracting data from file\n",
    "with open(\"fold436.train\") as f:\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        else:\n",
    "            line = line.strip()\n",
    "            data_train.append(line)\n",
    "\n",
    "# Extracting data from file\n",
    "with open(\"fold436.test\") as f:\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        else:\n",
    "            line = line.strip()\n",
    "            data_test.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Java reserved words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reserved_words = list()\n",
    "\n",
    "with open(\"java_words.txt\") as f:\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        else:\n",
    "            line = line.strip()\n",
    "            reserved_words.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data: list):\n",
    "    processed_data = list()\n",
    "    \n",
    "    for item in data:\n",
    "        tokens = item.split()\n",
    "        processed = list()\n",
    "        \n",
    "        for token in tokens:\n",
    "            if not token in reserved_words and token.isalpha():\n",
    "                processed.append(token)\n",
    "        processed_data.append(\" \".join(processed))\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "data_train = preprocess_data(data_train)\n",
    "data_test = preprocess_data(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting entropy values for the unigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probabilities_unigram(data: list):\n",
    "    data_join = \" \".join(data)\n",
    "    data_splitted = data_join.split()\n",
    "    \n",
    "    counter_names = dict(Counter(data_splitted))\n",
    "    keys = list(counter_names.keys())\n",
    "    total = sum(counter_names.values())\n",
    "\n",
    "    probabilities = list(map(lambda number: number / total, counter_names.values()))\n",
    "    probabilities_words = list(map(lambda word: probabilities[keys.index(word)], data_splitted))\n",
    "    \n",
    "    return probabilities_words\n",
    "\n",
    "def get_entropy_unigram(sentence: list, data: list, probabilities: list):\n",
    "    data_join = \" \".join(data)\n",
    "    data_splitted = data_join.split()\n",
    "    \n",
    "    sentence_join = \" \".join(sentence)\n",
    "    sentence_splitted = sentence_join.split()\n",
    "    \n",
    "    entropy_values = list()\n",
    "    \n",
    "    for token in sentence_splitted:\n",
    "        count = data_splitted.count(token)\n",
    "        if count:\n",
    "            probability = probabilities[data_splitted.index(token)]\n",
    "            entropy_values.append(probability * np.log2(probability))\n",
    "        else:\n",
    "            entropy_values.append(0)\n",
    "\n",
    "    return -1 * sum(entropy_values)\n",
    "\n",
    "probabilities_unigram = get_probabilities_unigram(data_train)\n",
    "entropy_unigram = get_entropy_unigram(data_test, data_train, probabilities_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.24369647410284"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the entropy values for the bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probabilities_bigram(data: list):\n",
    "    keys = list()\n",
    "    counter = list()\n",
    "    probabilities = list()\n",
    "    \n",
    "    for items in data:\n",
    "        tokens = items.split()\n",
    "        for i in range(len(tokens) - 1):\n",
    "            couple = tokens[i] + \" \" + tokens[i + 1]\n",
    "            keys.append(couple)\n",
    "    \n",
    "    counter_keys = dict(Counter(keys))\n",
    "\n",
    "    data_join = \" \".join(data)\n",
    "    data_all_split = data_join.split()\n",
    "    counter_names = dict(Counter(data_all_split))\n",
    "    \n",
    "    for key, value in counter_keys.items():\n",
    "        probability = value / counter_names[key.split()[0]]\n",
    "        probabilities.append(probability)\n",
    "    \n",
    "    return list(counter_keys.keys()), probabilities\n",
    "\n",
    "def get_indices(keys: list, token: str):\n",
    "    indices = list()\n",
    "    \n",
    "    for i in range(len(keys)):\n",
    "        key = keys[i]\n",
    "        first_half = key.split()[0]\n",
    "        if first_half == token:\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "\n",
    "def get_entropy_bigram(sentence: list,\n",
    "                       data: list,\n",
    "                       keys: list,\n",
    "                       probabilities_unigram: list,\n",
    "                       probabilities_bigram: list):\n",
    "\n",
    "    data_join = \" \".join(data)\n",
    "    data_all_split = data_join.split()\n",
    "    \n",
    "    entropy_values = list()\n",
    "    \n",
    "    for items in sentence:\n",
    "        tokens = items.split()\n",
    "        for i in range(len(tokens) - 1):\n",
    "            couple = tokens[i] + \" \" + tokens[i + 1]\n",
    "            if couple in keys:\n",
    "                probability_unigram = probabilities_unigram[data_all_split.index(tokens[i])]\n",
    "\n",
    "                indices = get_indices(keys, tokens[i])\n",
    "                probs_bigram = np.array([probabilities_bigram[index] for index in indices])\n",
    "                \n",
    "                partial_result = probability_unigram * sum(probs_bigram * np.log2(probs_bigram))\n",
    "                entropy_values.append(partial_result)\n",
    "            else:\n",
    "                entropy_values.append(0)\n",
    "    return -1 * sum(entropy_values)\n",
    "\n",
    "keys_bigram, probabilities_bigram = get_probabilities_bigram(data_train)\n",
    "entropy_bigram = get_entropy_bigram(data_test, data_train, keys_bigram, probabilities_unigram, probabilities_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.578933378491214"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the entropy values for the trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probabilities_trigram(data: list):\n",
    "    keys = list()\n",
    "    counter = list()\n",
    "    probabilities = list()\n",
    "    \n",
    "    for items in data:\n",
    "        tokens = items.split()\n",
    "        for i in range(len(tokens) - 2):\n",
    "            triple = tokens[i] + \" \" + tokens[i + 1] + \" \" + tokens[i + 2]\n",
    "            keys.append(triple)\n",
    "\n",
    "    counter_keys = dict(Counter(keys))\n",
    "\n",
    "    data_join = \" \".join(data)\n",
    "    data_all_split = data_join.split()\n",
    "    counter_names = dict(Counter(data_all_split))\n",
    "    \n",
    "    for key, value in counter_keys.items():\n",
    "        probability = value / counter_names[key.split()[0]]\n",
    "        probabilities.append(probability)\n",
    "    \n",
    "    return list(counter_keys.keys()), probabilities\n",
    "\n",
    "\n",
    "def get_entropy_trigram(sentence: list,\n",
    "                       data: list,\n",
    "                       keys: list,\n",
    "                       keys_bigram: list,\n",
    "                       probabilities_unigram: list,\n",
    "                       probabilities_bigram: list,\n",
    "                       probabilities_trigram: list):\n",
    "\n",
    "    data_join = \" \".join(data)\n",
    "    data_all_split = data_join.split()\n",
    "    \n",
    "    entropy_values = list()\n",
    "    \n",
    "    for items in sentence:\n",
    "        tokens = items.split()\n",
    "        for i in range(len(tokens) - 2):\n",
    "            triple = tokens[i] + \" \" + tokens[i + 1] + \" \" + tokens[i + 2]\n",
    "            if triple in keys:\n",
    "                probability_unigram = probabilities_unigram[data_all_split.index(tokens[i])]\n",
    "                probability_bigram = probabilities_bigram[keys_bigram.index(tokens[i] + \" \" + tokens[i + 1])]\n",
    "\n",
    "                indices = get_indices(keys, tokens[i])\n",
    "                probs_trigram = np.array([probabilities_trigram[index] for index in indices])\n",
    "                \n",
    "                partial_result = probability_unigram * probability_bigram * sum(probs_trigram * np.log2(probs_trigram))\n",
    "                entropy_values.append(partial_result)\n",
    "            else:\n",
    "                entropy_values.append(0)\n",
    "    return -1 * sum(entropy_values)\n",
    "\n",
    "keys_trigram, probabilities_trigram = get_probabilities_trigram(data_train)\n",
    "entropy_trigram = get_entropy_trigram(data_test, data_train, keys_trigram, keys_bigram, probabilities_unigram, probabilities_bigram, probabilities_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8464322432160059"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_trigram"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
